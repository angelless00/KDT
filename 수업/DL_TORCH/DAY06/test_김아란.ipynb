{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.퍼셉트론 개념에 대해 설명하시오 \n",
    "- 레이어 속에 존재하는 하나의 식이나오는 한개의 신경망"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 퍼셉트론의 기본 동작 원리 및 수식을 도식화와 함꼐 작성해주세요.\n",
    "- 피쳐 4개 $x_1,x_2,x_3,x_4$ 에 대하여 $w_1x_1+w_2x_2+w_3x_3+w_4x_4+b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 활성화함수의 역할을 설명하세요\n",
    "- 레이어 사이의 연결고리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 대표적인 활성화 함수에 대해 설명하세요\n",
    "- 시그모이드 $ -1 <= x <= +1$\n",
    "- ReLU $0,x$\n",
    "- leaky ReLU..? $0.01x , x$\n",
    "- $ p_i / \\Sigma p_k $ .......이름이 뭐엿드라.. 소프트맥스"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 회귀 모델 구현을 간략하게 코드 작성하세요  \n",
    "피쳐3개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[85.63208770751953, 70.50347900390625, 58.47321701049805, 48.897850036621094, 42.12773513793945]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "from torchinfo  import summary\n",
    "from torchmetrics.regression import MeanSquaredError\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "featureDF=pd.DataFrame(np.arange(30).reshape(-1,3))\n",
    "targetDF=pd.DataFrame(np.arange(10).reshape(-1,1))\n",
    "\n",
    "class RegModel(nn.Module):\n",
    "    def __init__(self,in_in,out_out,n1,n2):\n",
    "        super().__init__()\n",
    "        self.in_layer=nn.Linear(in_in,n1)\n",
    "        self.h_layer=nn.Linear(n1,n2)\n",
    "        self.out_layer=nn.Linear(n2,out_out)   \n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.in_layer(x)\n",
    "        x=F.relu(x)\n",
    "        x=self.h_layer(x)\n",
    "        x=F.relu(x)\n",
    "        return self.out_layer(x)\n",
    "\n",
    "\n",
    "class RegDataSet(Dataset):\n",
    "    def __init__(self,feature,target):\n",
    "        super().__init__()\n",
    "        self.feature=feature\n",
    "        self.target=target\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.feature.shape[0]\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        featureTS=torch.FloatTensor(featureDF.iloc[index].values)\n",
    "        targetTS=torch.FloatTensor(targetDF.iloc[index].values)\n",
    "        return featureTS,targetTS\n",
    "\n",
    "in_in=featureDF.shape[1]\n",
    "out_out=targetDF.shape[1]\n",
    "EPOCH=5\n",
    "BATCH_SIZE=5\n",
    "dataDS=RegDataSet(featureDF,targetDF)\n",
    "dataDL=DataLoader(dataDS,batch_size=BATCH_SIZE)\n",
    "model=RegModel(in_in,out_out,2,5)\n",
    "optimizer=optim.Adam(model.parameters(),lr=0.01)\n",
    "\n",
    "loss_hist=[]\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    loss_total=0\n",
    "    for feature,target in dataDL:\n",
    "        model.train()\n",
    "        pre_y=model(feature)\n",
    "\n",
    "        loss=MeanSquaredError()(pre_y,target)\n",
    "        loss_total+=loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    loss_hist.append((loss_total.item())/len(dataDL))\n",
    "\n",
    "print(loss_hist)\n",
    "       \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 이진분류 모델 구현을 간략하게 코드 작성하세요  \n",
    "피쳐 5개 클래스 2개 층4개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not function",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 61\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature,target \u001b[38;5;129;01min\u001b[39;00m dataDL:\n\u001b[0;32m     60\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 61\u001b[0m     pre_y\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m     loss\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mBCELoss()(pre_y,target)\n\u001b[0;32m     64\u001b[0m     loss_total\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mloss\n",
      "File \u001b[1;32mc:\\Users\\KDP-30\\anaconda3\\envs\\TORCH_38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KDP-30\\anaconda3\\envs\\TORCH_38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[51], line 27\u001b[0m, in \u001b[0;36mBiClassModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     25\u001b[0m x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh1_layer(x)\n\u001b[0;32m     26\u001b[0m x\u001b[38;5;241m=\u001b[39mF\u001b[38;5;241m.\u001b[39mrelu\n\u001b[1;32m---> 27\u001b[0m x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mh2_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m x\u001b[38;5;241m=\u001b[39mF\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_layer(x)\n",
      "File \u001b[1;32mc:\\Users\\KDP-30\\anaconda3\\envs\\TORCH_38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KDP-30\\anaconda3\\envs\\TORCH_38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KDP-30\\anaconda3\\envs\\TORCH_38\\lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not function"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "from torchinfo  import summary\n",
    "from torchmetrics.classification import BinaryF1Score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "featureDF=pd.DataFrame(np.arange(30).reshape(-1,5))\n",
    "targetDF=pd.DataFrame(np.random.randint(0,2,6))\n",
    "\n",
    "class BiClassModel(nn.Module):\n",
    "    def __init__(self,in_in,out_out,n1,n2,n3):\n",
    "        super().__init__()\n",
    "        self.in_layer=nn.Linear(in_in,n1)\n",
    "        self.h1_layer=nn.Linear(n1,n2)\n",
    "        self.h2_layer=nn.Linear(n1,n3)\n",
    "        self.out_layer=nn.Linear(n3,out_out)   \n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.in_layer(x)\n",
    "        x=F.relu(x)\n",
    "        x=self.h1_layer(x)\n",
    "        x=F.relu\n",
    "        x=self.h2_layer(x)\n",
    "        x=F.relu(x)\n",
    "        return self.out_layer(x)\n",
    "\n",
    "\n",
    "class RegDataSet(Dataset):\n",
    "    def __init__(self,feature,target):\n",
    "        super().__init__()\n",
    "        self.feature=feature\n",
    "        self.target=target\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.feature.shape[0]\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        featureTS=torch.FloatTensor(featureDF.iloc[index].values)\n",
    "        targetTS=torch.FloatTensor(targetDF.iloc[index].values)\n",
    "        return featureTS,targetTS\n",
    "\n",
    "in_in=featureDF.shape[1]\n",
    "out_out=targetDF.shape[1]\n",
    "EPOCH=5\n",
    "BATCH_SIZE=5\n",
    "dataDS=RegDataSet(featureDF,targetDF)\n",
    "dataDL=DataLoader(dataDS,batch_size=BATCH_SIZE)\n",
    "model=BiClassModel(in_in,out_out,2,3,5)\n",
    "optimizer=optim.Adam(model.parameters(),lr=0.01)\n",
    "\n",
    "loss_hist=[]\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    loss_total=0\n",
    "    for feature,target in dataDL:\n",
    "        model.train()\n",
    "        pre_y=model(feature)\n",
    "\n",
    "        loss=nn.BCELoss()(pre_y,target)\n",
    "        loss_total+=loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    loss_hist.append((loss_total.item())/len(dataDL))\n",
    "\n",
    "print(loss_hist)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. 다중분류모델구현을 간략하게 코드 작성하세요 .  \n",
    "피쳐 5개 클래스 8개 층 3~5 퍼셉트론 동적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "append() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 52\u001b[0m\n\u001b[0;32m     50\u001b[0m dataDS\u001b[38;5;241m=\u001b[39mRegDataSet(featureDF,targetDF)\n\u001b[0;32m     51\u001b[0m dataDL\u001b[38;5;241m=\u001b[39mDataLoader(dataDS,batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE)\n\u001b[1;32m---> 52\u001b[0m model\u001b[38;5;241m=\u001b[39m\u001b[43mMultiClassModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43mout_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m optimizer\u001b[38;5;241m=\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(),lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m     55\u001b[0m loss_hist\u001b[38;5;241m=\u001b[39m[]\n",
      "Cell \u001b[1;32mIn[53], line 20\u001b[0m, in \u001b[0;36mMultiClassModel.__init__\u001b[1;34m(self, in_in, out_out, n)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh_layer\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mModuleList()\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(n)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mh_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_layer\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear(n[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],out_out)\n",
      "\u001b[1;31mTypeError\u001b[0m: append() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "from torchinfo  import summary\n",
    "from torchmetrics.classification import MulticlassF1Score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "featureDF=pd.DataFrame(np.arange(30).reshape(-1,5))\n",
    "targetDF=pd.DataFrame(np.random.randint(0,8,6))\n",
    "\n",
    "class MultiClassModel(nn.Module):\n",
    "    def __init__(self,in_in,out_out,n=[]):\n",
    "        super().__init__()\n",
    "        self.in_layer=nn.Linear(in_in,n[0])\n",
    "        self.h_layer=nn.ModuleList()\n",
    "        for i in range(len(n)-1):\n",
    "            self.h_layer.append(n[i],n[i+1])\n",
    "        self.out_layer=nn.Linear(n[-1],out_out)   \n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.in_layer(x)\n",
    "        x=F.relu(x)\n",
    "        for model in self.h_layer:\n",
    "            x=F.relu(model(x))\n",
    "        x=F.relu(x)\n",
    "        return self.out_layer(x)\n",
    "\n",
    "\n",
    "class RegDataSet(Dataset):\n",
    "    def __init__(self,feature,target):\n",
    "        super().__init__()\n",
    "        self.feature=feature\n",
    "        self.target=target\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.feature.shape[0]\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        featureTS=torch.FloatTensor(featureDF.iloc[index].values)\n",
    "        targetTS=torch.FloatTensor(targetDF.iloc[index].values)\n",
    "        return featureTS,targetTS\n",
    "\n",
    "in_in=featureDF.shape[1]\n",
    "out_out=targetDF.shape[1]\n",
    "EPOCH=5\n",
    "BATCH_SIZE=5\n",
    "dataDS=RegDataSet(featureDF,targetDF)\n",
    "dataDL=DataLoader(dataDS,batch_size=BATCH_SIZE)\n",
    "model=MultiClassModel(in_in,out_out,n=[5,4,9,5])\n",
    "optimizer=optim.Adam(model.parameters(),lr=0.01)\n",
    "\n",
    "loss_hist=[]\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    loss_total=0\n",
    "    for feature,target in dataDL:\n",
    "        model.train()\n",
    "        pre_y=model(feature)\n",
    "\n",
    "        loss=nn.BCELoss()(pre_y,target)\n",
    "        loss_total+=loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    loss_hist.append((loss_total.item())/len(dataDL))\n",
    "\n",
    "print(loss_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. 기울기 소실 개념 및 해결방법을 설명하세요.\n",
    "- 활성화함수 사용시 기울기가 0 으로 수렴하게 되는 상황이 발생 ReLU로 해결"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. 파이토치의 모델 동작 모드에 대 해관려 함수도 함께 설명하시오.\n",
    "- model.train() \n",
    "    - 학습모드, 가중치의 변동과 다음스텝진행가능 \n",
    "- model.eval()\n",
    "    - 검증모드, 가중치의 변동을 멈추고 검증이나, 테스트를 위해 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

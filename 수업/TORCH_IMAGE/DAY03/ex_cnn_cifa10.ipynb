{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 이미지 분류 모델\n",
    "- 이미지 데이터셋 준비\n",
    "    - torhchvision의 내장 데이터셋 활용 CIFIA10\n",
    "- 이미지 분류 모델\n",
    "    - 커스텀 CNN 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] 모듈 로딩 및 데이터 준비 <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "from torchvision.datasets import CIFAR10        # torchvison 내장데이터셋\n",
    "from torchvision.transforms import ToTensor     # torchvision 데이터 변환 관련 모듈\n",
    "from torch.utils.data import DataLoader         # 데이터셋 관련 모듈 로딩\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 데이터 로딩\n",
    "DIR_PATH='../data/'\n",
    "\n",
    "# Pytorch의 Dataset 형태 로딩\n",
    "cifarDS=CIFAR10(DIR_PATH,\n",
    "                train=True,\n",
    "                download=False,\n",
    "                transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.cifar.CIFAR10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cifarDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes : ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
      " class to idx : {'airplane': 0, 'automobile': 1, 'bird': 2, 'cat': 3, 'deer': 4, 'dog': 5, 'frog': 6, 'horse': 7, 'ship': 8, 'truck': 9}\n",
      " shape : (50000, 32, 32, 3)\n",
      " targets length : 50000\n"
     ]
    }
   ],
   "source": [
    "# Dataset의 속성 확인\n",
    "print(f'classes : {cifarDS.classes}')\n",
    "print(F' class to idx : {cifarDS.class_to_idx}')\n",
    "print(F' shape : {cifarDS.data.shape}')\n",
    "print(F' targets length : {len(cifarDS.targets)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2] 데이터 로더 <hr>\n",
    "- 학습시 배치크기만큼 데이터와 라벨/타겟을 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=50\n",
    "\n",
    "cifar10DL=DataLoader(cifarDS,batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 32, 32])\n",
      "tensor([6, 9, 9, 4, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\n데이터 로더 사용시 채널이 앞으로 옴!\\ndata. numpy() < array로 바꾸고\\n그림을 그리고 싶다면 젤 앞에 있는 1을 지우고(squeeze)\\n채널을 뒤로 밀어야함 (data.T)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for data,target in cifar10DL:\n",
    "    print(data.shape)\n",
    "    print(target)\n",
    "    #print(data.numpy())\n",
    "    # plt.imshow(data.squeeze().transpose(0,2))\n",
    "    # plt.title(cifarDS.classes[target.item()])\n",
    "    break\n",
    "\n",
    "\"\"\" \n",
    "데이터 로더 사용시 채널이 앞으로 옴!\n",
    "data. numpy() < array로 바꾸고\n",
    "그림을 그리고 싶다면 젤 앞에 있는 1을 지우고(squeeze)\n",
    "채널을 뒤로 밀어야함 (data.T)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3] 커스텀 모델 설계 및 정의 <hr>\n",
    "- 모델목적 : 이미지 분류 모델\n",
    "- 학습방법 : 지도학습 > 분류 > 다중분류 (10개)\n",
    "- 클래스이름 : ImageMCF\n",
    "- 클래스구조 : 특징추출부분 => CNN + 학습부분 FC\n",
    "- 부모클래스 : nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageMCF(nn.Module):\n",
    "    # 모델구조 설계 즉, 생성자 메서드\n",
    "    def __init__(self):\n",
    "        # 부모 생성\n",
    "        super().__init__()\n",
    "        # 모델 층 구성\n",
    "        # 특징 추출 층\n",
    "        self.cnn_layer=nn.Sequential(\n",
    "            nn.Conv2d(3,10,kernel_size=3),\n",
    "            nn.BatchNorm2d(10),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "\n",
    "        # [1, 3, 32, 32] : input\n",
    "        self.in_layer=nn.Conv2d(3,10,3)\n",
    "        # [1, 10, 30, 30]\n",
    "        self.p_layer=nn.MaxPool2d(2)\n",
    "        # [1, 10, 15, 15]\n",
    "\n",
    "        # 학습 관련 층\n",
    "        self.hd_layer=nn.Linear(10*15*15 ,20)\n",
    "        self.out_layer=nn.Linear(20 ,10)\n",
    "\n",
    "    # 전방향/순방향 학습 메서드\n",
    "    def forward(self,input):\n",
    "        # 이미지 틍징 맵 추출\n",
    "        output=self.in_layer(input)\n",
    "        print(f'[output1] {output.shape}')\n",
    "\n",
    "        output=F.relu(output)\n",
    "        print(f'[output2] {output.shape}')\n",
    "\n",
    "        output=self.p_layer(output)\n",
    "        print(f'[output3] {output.shape}')\n",
    "\n",
    "        # 4D -> 2D (샘플수, 피처수)\n",
    "        output=output.view(output.shape[0],-1)\n",
    "        print(f'[output4] {output.shape}')\n",
    "\n",
    "        output=F.relu(self.hd_layer(output))\n",
    "        print(f'[output5] {output.shape}')\n",
    "\n",
    "        output=self.out_layer(output)\n",
    "        print(f'[output6] {output.shape}')\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageMCF(nn.Module):\n",
    "    # 모델구조 설계 즉, 생성자 메서드\n",
    "    def __init__(self):\n",
    "        # 부모 생성\n",
    "        super().__init__()\n",
    "        # 모델 층 구성\n",
    "        # 특징 추출 층\n",
    "        self.cnn_layer=nn.Sequential(\n",
    "            nn.Conv2d(3,10,kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.cnn_layer2=nn.Sequential(\n",
    "            nn.Conv2d(10,30,kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.cnn_layer2=nn.Sequential(\n",
    "            nn.Conv2d(10,30,kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(30,50,kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(2))                    # 마지막엔 AVGpool\n",
    "\n",
    "        # 학습 관련 층\n",
    "        self.hd_layer=nn.Linear(50*7*7 ,20)\n",
    "        self.out_layer=nn.Linear(20 ,10)\n",
    "\n",
    "    # 전방향/순방향 학습 메서드\n",
    "    def forward(self,input):\n",
    "        # 이미지 틍징 맵 추출\n",
    "        output=self.cnn_layer(input)\n",
    "\n",
    "        # 4D -> 2D (샘플수, 피처수)\n",
    "        output=output.view(output.shape[0],-1)\n",
    "        print(f'[output4] {output.shape}')\n",
    "\n",
    "        output=F.relu(self.hd_layer(output))\n",
    "        print(f'[output5] {output.shape}')\n",
    "\n",
    "        output=self.out_layer(output)\n",
    "        print(f'[output6] {output.shape}')\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "ImageMCF                                 --\n",
       "├─Sequential: 1-1                        --\n",
       "│    └─Conv2d: 2-1                       280\n",
       "│    └─ReLU: 2-2                         --\n",
       "│    └─MaxPool2d: 2-3                    --\n",
       "├─Sequential: 1-2                        --\n",
       "│    └─Conv2d: 2-4                       2,730\n",
       "│    └─ReLU: 2-5                         --\n",
       "│    └─Conv2d: 2-6                       13,550\n",
       "│    └─ReLU: 2-7                         --\n",
       "│    └─AvgPool2d: 2-8                    --\n",
       "├─Linear: 1-3                            49,020\n",
       "├─Linear: 1-4                            210\n",
       "=================================================================\n",
       "Total params: 65,790\n",
       "Trainable params: 65,790\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "m=ImageMCF()\n",
    "summary(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[target]==> tensor([6, 9, 9, 4, 1])\n",
      "torch.Size([5, 3, 32, 32])\n",
      "[output4] torch.Size([5, 2250])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (5x2250 and 2450x20)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 학습\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m pre\u001b[38;5;241m=\u001b[39m\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(pre)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpre => \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpre\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\KDP-30\\anaconda3\\envs\\TORCH_CV_38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KDP-30\\anaconda3\\envs\\TORCH_CV_38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[9], line 36\u001b[0m, in \u001b[0;36mImageMCF.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     33\u001b[0m output\u001b[38;5;241m=\u001b[39moutput\u001b[38;5;241m.\u001b[39mview(output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[output4] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m output\u001b[38;5;241m=\u001b[39mF\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhd_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[output5] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     39\u001b[0m output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_layer(output)\n",
      "File \u001b[1;32mc:\\Users\\KDP-30\\anaconda3\\envs\\TORCH_CV_38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KDP-30\\anaconda3\\envs\\TORCH_CV_38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KDP-30\\anaconda3\\envs\\TORCH_CV_38\\lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (5x2250 and 2450x20)"
     ]
    }
   ],
   "source": [
    "for data,target in cifar10DL:\n",
    "    print(f'[target]==> {target}')\n",
    "    print(data.shape)\n",
    "    # 학습\n",
    "    pre=m(data)\n",
    "    print(pre)\n",
    "    print(f'pre => {pre.argmax(dim=1)}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[in_layer.weight]-------- \n",
      "torch.Size([10, 3, 3, 3])\n",
      "[in_layer.bias]-------- \n",
      "torch.Size([10])\n",
      "[hd_layer.weight]-------- \n",
      "torch.Size([20, 2250])\n",
      "[hd_layer.bias]-------- \n",
      "torch.Size([20])\n",
      "[out_layer.weight]-------- \n",
      "torch.Size([10, 20])\n",
      "[out_layer.bias]-------- \n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name,param in m.named_parameters():\n",
    "    print(f'[{name}]-------- \\n{param.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_CV_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 분류 모델\n",
    "from torch import nn\n",
    "\n",
    "class SentenceClassifier(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_vocab,\n",
    "            hidden_dim,\n",
    "            embedding_dim,\n",
    "            n_layers,\n",
    "            dropout=0.5,\n",
    "            bidirectional=True,\n",
    "            model_type='lstm'\n",
    "            ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding=nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=0\n",
    "        )\n",
    "        if model_type=='rnn':\n",
    "            self.model=nn.RNN(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True\n",
    "            )\n",
    "        elif model_type=='lstm':\n",
    "            self.model=nn.LSTM(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True   \n",
    "            )\n",
    "        \n",
    "        if bidirectional:\n",
    "            self.classifier=nn.Linear(hidden_dim*2,1)\n",
    "        else:\n",
    "            self.classifier=nn.Linear(hidden_dim,1)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        embeddings=self.embedding(inputs)\n",
    "        output,_=self.model(embeddings)\n",
    "        last_output=output[:,-1,:]\n",
    "        last_output=self.dropout(last_output)\n",
    "        logits=self.classifier(last_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\KDP-30\\Korpora\\nsmc\\ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\KDP-30\\Korpora\\nsmc\\ratings_test.txt\n",
      "|       | text                                                                                     |   label |\n",
      "|------:|:-----------------------------------------------------------------------------------------|--------:|\n",
      "| 33553 | 모든 편견을 날려 버리는 가슴 따뜻한 영화. 로버트 드 니로, 필립 세이모어 호프만 영원하라. |       1 |\n",
      "|  9427 | 무한 리메이크의 소재. 감독의 역량은 항상 그 자리에...                                    |       0 |\n",
      "|   199 | 신날 것 없는 애니.                                                                       |       0 |\n",
      "| 12447 | 잔잔 격동                                                                                |       1 |\n",
      "| 39489 | 오랜만에 찾은 주말의 명화의 보석                                                         |       1 |\n",
      "Training Data Size : 45000\n",
      "Testing Data Size : 5000\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 불러오기\n",
    "import pandas as pd\n",
    "from Korpora import Korpora\n",
    "\n",
    "corpus=Korpora.load('nsmc')\n",
    "corpus_df=pd.DataFrame(corpus.test)\n",
    "\n",
    "train=corpus_df.sample(frac=0.9,random_state=42)\n",
    "test=corpus_df.drop(train.index)\n",
    "\n",
    "print(train.head(5).to_markdown())\n",
    "print('Training Data Size :',len(train))\n",
    "print('Testing Data Size :',len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<unk>', '.', '이', '영화', '의', '..', '가', '에', '...']\n",
      "5002\n"
     ]
    }
   ],
   "source": [
    "# 데이터 토큰화 및 단어사전 구축\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "\n",
    "def build_vocab(corpus,n_vocab,special_tokens):\n",
    "    counter=Counter()\n",
    "    for tokens in corpus:\n",
    "        counter.update(tokens)\n",
    "    vocab=special_tokens\n",
    "    for token,count in counter.most_common(n_vocab):\n",
    "        vocab.append(token)\n",
    "    return vocab\n",
    "\n",
    "tokenizer=Okt()\n",
    "train_tokens=[tokenizer.morphs(review) for review in train.text]\n",
    "test_tokens=[tokenizer.morphs(review) for review in test.text]\n",
    "\n",
    "vocab=build_vocab(corpus=train_tokens, n_vocab=5000,special_tokens=['<pad>','<unk>'])\n",
    "token_to_id={token:idx for idx,token in enumerate(vocab)}\n",
    "id_to_token={idx:token for idx,token in enumerate(vocab)}\n",
    "\n",
    "print(vocab[:10])\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 223 1716   10 4036 2095  193  755    4    2 2330 1031  220   26   13\n",
      " 4839    1    1    1    2    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n",
      "[3307    5 1997  456    8    1 1013 3906    5    1    1   13  223   51\n",
      "    3    1 4684    6    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# 정수 인코딩 및 패딩\n",
    "import numpy as np\n",
    "\n",
    "def pad_sequences(sequences,max_length,pad_value):\n",
    "    result=list()\n",
    "    for sequence in sequences:\n",
    "        sequence=sequence[:max_length]\n",
    "        pad_length=max_length - len(sequence)\n",
    "        padded_sequence=sequence+[pad_value]*pad_length\n",
    "        result.append(padded_sequence)\n",
    "    return np.asarray(result)\n",
    "\n",
    "unk_id=token_to_id['<unk>']\n",
    "train_ids=[[token_to_id.get(token,unk_id) for token in review] for review in train_tokens]\n",
    "test_ids=[[token_to_id.get(token,unk_id) for token in review] for review in test_tokens]\n",
    "\n",
    "max_length=32\n",
    "pad_id=token_to_id['<pad>']\n",
    "train_ids=pad_sequences(train_ids,max_length,pad_id)\n",
    "test_ids=pad_sequences(test_ids,max_length,pad_id)\n",
    "\n",
    "print(train_ids[0])\n",
    "print(test_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로더 적용\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "\n",
    "train_ids=torch.tensor(train_ids)\n",
    "test_ids=torch.tensor(test_ids)\n",
    "\n",
    "train_labels=torch.tensor(train.label.values,dtype=torch.float32)\n",
    "test_labels=torch.tensor(test.label.values,dtype=torch.float32)\n",
    "\n",
    "train_dataset=TensorDataset(train_ids,train_labels)\n",
    "test_dataset=TensorDataset(test_ids,test_labels)\n",
    "\n",
    "train_loader=DataLoader(train_dataset,batch_size=16,shuffle=True)\n",
    "test_loader=DataLoader(test_dataset,batch_size=16,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수와 최적화 함수 정의\n",
    "from torch import optim\n",
    "\n",
    "n_vocab=len(token_to_id)\n",
    "hidden_dim=64\n",
    "embedding_dim=128\n",
    "n_layers=2\n",
    "\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "classifier=SentenceClassifier(n_vocab=n_vocab,hidden_dim=hidden_dim,embedding_dim=embedding_dim,n_layers=n_layers).to(device)\n",
    "criterion=nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer=optim.RMSprop(classifier.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 0 : 0.7056640386581421\n",
      "Train Loss 500 : 0.693762791965774\n",
      "Train Loss 1000 : 0.6826789934080202\n",
      "Train Loss 1500 : 0.6739627492578724\n",
      "Train Loss 2000 : 0.6652488444579475\n",
      "Train Loss 2500 : 0.6575580659221526\n",
      "Val Loss : 0.6006449655222055, Val Accuracy : 0.705\n",
      "Train Loss 0 : 0.689236044883728\n",
      "Train Loss 500 : 0.6013172161198423\n",
      "Train Loss 1000 : 0.5943840187448602\n",
      "Train Loss 1500 : 0.5943782634134693\n",
      "Train Loss 2000 : 0.5864239292464096\n",
      "Train Loss 2500 : 0.5783238524510735\n",
      "Val Loss : 0.5395869973558969, Val Accuracy : 0.7342\n",
      "Train Loss 0 : 0.6035038828849792\n",
      "Train Loss 500 : 0.5283326465272142\n",
      "Train Loss 1000 : 0.5158798949165897\n",
      "Train Loss 1500 : 0.5110080776652998\n",
      "Train Loss 2000 : 0.5011310465093972\n",
      "Train Loss 2500 : 0.49235118276736395\n",
      "Val Loss : 0.4467549383068999, Val Accuracy : 0.7932\n",
      "Train Loss 0 : 0.5440927147865295\n",
      "Train Loss 500 : 0.42414094986553913\n",
      "Train Loss 1000 : 0.4210331855671151\n",
      "Train Loss 1500 : 0.41656882060737627\n",
      "Train Loss 2000 : 0.4135403377541538\n",
      "Train Loss 2500 : 0.4104460358756726\n",
      "Val Loss : 0.4052556631283257, Val Accuracy : 0.8176\n",
      "Train Loss 0 : 0.3150469958782196\n",
      "Train Loss 500 : 0.36120895195209574\n",
      "Train Loss 1000 : 0.3645503079215368\n",
      "Train Loss 1500 : 0.35967017217309055\n",
      "Train Loss 2000 : 0.35889499563759175\n",
      "Train Loss 2500 : 0.35883597012271123\n",
      "Val Loss : 0.39307514397195353, Val Accuracy : 0.8242\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습 및 테스트\n",
    "def train(model,datasets,criterion,optimizer,device,interval):\n",
    "    model.train()\n",
    "    losses=list()\n",
    "\n",
    "    for step,(input_ids,labels) in enumerate(datasets):\n",
    "        input_ids=input_ids.to(device)\n",
    "        labels=labels.to(device).unsqueeze(1)\n",
    "\n",
    "        logits=model(input_ids)\n",
    "        loss=criterion(logits,labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step%interval==0:\n",
    "            print(f'Train Loss {step} : {np.mean(losses)}')\n",
    "\n",
    "def test(model,datasets,criterion,device):\n",
    "    model.eval()\n",
    "    losses=list()\n",
    "    corrects=list()\n",
    "\n",
    "    for step,(input_ids,labels) in enumerate(datasets):\n",
    "        input_ids=input_ids.to(device)\n",
    "        labels=labels.to(device).unsqueeze(1)\n",
    "\n",
    "        logits=model(input_ids)\n",
    "        loss=criterion(logits,labels)\n",
    "        losses.append(loss.item())\n",
    "        yhat=torch.sigmoid(logits)>.5\n",
    "        corrects.extend(\n",
    "            torch.eq(yhat,labels).cpu().tolist()\n",
    "        )\n",
    "\n",
    "    print(f'Val Loss : {np.mean(losses)}, Val Accuracy : {np.mean(corrects)}')\n",
    "\n",
    "epochs=5\n",
    "interval=500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(classifier,train_loader,criterion,optimizer,device,interval)\n",
    "    test(classifier,test_loader,criterion,device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "보고싶다 [-1.8683641  -1.3740939   1.7592905   0.70835006 -0.29292408  0.24323533\n",
      " -1.2669592   1.0632653   0.42124623  1.0604889  -0.8373948  -0.7002585\n",
      "  0.35543796  0.35817844  1.6392184  -1.3335398   0.51421636 -0.22712448\n",
      "  0.83719736  0.59881765 -0.54472256 -0.14003327 -1.089459   -0.6348388\n",
      " -1.42152    -1.0494287  -1.1649648   1.296978    0.9583308  -0.31751117\n",
      " -0.24820495 -0.26237667 -0.37054545  0.26183638  0.5348833   0.40665674\n",
      "  0.535054   -0.26458335  0.92567116 -1.615079    0.39977413  1.1322653\n",
      " -0.7304918  -0.19022855  0.8435572  -0.3632555   1.2237774   1.711149\n",
      " -0.62056255  0.89880013 -0.77069217 -0.5066573  -0.22178869 -0.53092724\n",
      "  2.4233046   0.01304005 -1.1162593   0.65695375  0.01025385 -1.2377442\n",
      " -0.4059662  -1.1750962   1.316754    0.7121545   0.79570824  1.6789912\n",
      "  0.17559305 -0.78017986 -0.6577893  -0.27742603 -1.7648214   0.59902346\n",
      "  0.6311329  -0.3315085   0.7283992  -1.4376836   0.5626868   2.5795205\n",
      " -1.4623389  -1.097462   -0.6777411   0.4786321   0.7334677  -1.5610666\n",
      " -0.11218531  0.07898022 -0.49548736  0.969024    0.64507484 -0.82012594\n",
      " -0.08528617 -0.25122154  0.7535424   1.9671906   1.3204445   0.09498203\n",
      "  0.66069055 -0.08705961 -0.7028425   0.65319246  0.68670696 -0.67778593\n",
      " -0.93732053 -0.5401421  -0.8486817  -1.6893957   2.0375013  -0.7114632\n",
      " -0.24454544  1.1547709   0.5303636   0.49473003 -1.2844269  -0.18760912\n",
      "  0.00338863  0.10075451 -0.03142665  1.0023903   0.14302596  0.09091416\n",
      " -0.72505176 -0.07960819  1.581648    0.34441516 -0.10134598  0.8274705\n",
      " -0.6192355  -0.8803719 ]\n"
     ]
    }
   ],
   "source": [
    "# 학습된 모델로부터 임베딩 추출\n",
    "token_to_embedding=dict()\n",
    "embedding_matrix=classifier.embedding.weight.detach().cpu().numpy()\n",
    "\n",
    "for word,emb in zip(vocab,embedding_matrix):\n",
    "    token_to_embedding[word]=emb\n",
    "\n",
    "token=vocab[1000]\n",
    "print(token,token_to_embedding[token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "tokens=[tokenizer.morphs(review) for review in corpus_df.text]\n",
    "word2vec=Word2Vec(sentences=tokens, vector_size=128, window=5, min_count=1, sg=1, epochs=3, max_final_vocab=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('../models/') :\n",
    "    os.makedirs('../models/')\n",
    "    word2vec.save('../models/word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 학습된 모델로 임베딩 계층 초기화\n",
    "\n",
    "init_embeddings=np.zeros((n_vocab,embedding_dim))\n",
    "\n",
    "for index,token in id_to_token.items():\n",
    "    if token not in [\"<pad>\",'<unk>']:\n",
    "        init_embeddings[index]=word2vec.wv[token]\n",
    "\n",
    "embedding_layer=nn.Embedding.from_pretrained(torch.tensor(init_embeddings,dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceClassifier(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_vocab,\n",
    "            hidden_dim,\n",
    "            embedding_dim,\n",
    "            n_layers,\n",
    "            dropout=0.5,\n",
    "            bidirectional=True,\n",
    "            model_type='lstm',\n",
    "            pretrained_embedding=None\n",
    "            ):\n",
    "        super().__init__()\n",
    "        \n",
    "        if pretrained_embedding is not None:\n",
    "            self.embedding=nn.Embedding(\n",
    "                num_embeddings=n_vocab,\n",
    "                embedding_dim=embedding_dim,\n",
    "                padding_idx=0\n",
    "            )\n",
    "        else:\n",
    "            self.embedding=nn.Embedding(\n",
    "                num_embeddings=n_vocab,\n",
    "                embedding_dim=embedding_dim,\n",
    "                padding_idx=0\n",
    "            )\n",
    "        if model_type=='rnn':\n",
    "            self.model=nn.RNN(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True\n",
    "            )\n",
    "        elif model_type=='lstm':\n",
    "            self.model=nn.LSTM(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True   \n",
    "            )\n",
    "        \n",
    "        if bidirectional:\n",
    "            self.classifier=nn.Linear(hidden_dim*2,1)\n",
    "        else:\n",
    "            self.classifier=nn.Linear(hidden_dim,1)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        embeddings=self.embedding(inputs)\n",
    "        output,_=self.model(embeddings)\n",
    "        last_output=output[:,-1,:]\n",
    "        last_output=self.dropout(last_output)\n",
    "        logits=self.classifier(last_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 0 : 0.7030622959136963\n",
      "Train Loss 500 : 0.6936484206460432\n",
      "Train Loss 1000 : 0.6917418511240156\n",
      "Train Loss 1500 : 0.6782668178713694\n",
      "Train Loss 2000 : 0.6748790943968124\n",
      "Train Loss 2500 : 0.6659648386372037\n",
      "Val Loss : 0.622719238455684, Val Accuracy : 0.6744\n",
      "Train Loss 0 : 0.5098632574081421\n",
      "Train Loss 500 : 0.6272295207796459\n",
      "Train Loss 1000 : 0.6155383265935458\n",
      "Train Loss 1500 : 0.6050998080300936\n",
      "Train Loss 2000 : 0.5955927654810395\n",
      "Train Loss 2500 : 0.5872510355956456\n",
      "Val Loss : 0.524811508175664, Val Accuracy : 0.7572\n",
      "Train Loss 0 : 0.5801953673362732\n",
      "Train Loss 500 : 0.5152910835074331\n",
      "Train Loss 1000 : 0.5033478119841346\n",
      "Train Loss 1500 : 0.49367625631148776\n",
      "Train Loss 2000 : 0.4817994584528164\n",
      "Train Loss 2500 : 0.47329235025366895\n",
      "Val Loss : 0.42293687512318545, Val Accuracy : 0.8056\n",
      "Train Loss 0 : 0.361931174993515\n",
      "Train Loss 500 : 0.37341168623960425\n",
      "Train Loss 1000 : 0.3790219259577674\n",
      "Train Loss 1500 : 0.38388115345816226\n",
      "Train Loss 2000 : 0.38472909336608985\n",
      "Train Loss 2500 : 0.38721508219367456\n",
      "Val Loss : 0.4048005888065972, Val Accuracy : 0.815\n",
      "Train Loss 0 : 0.22955617308616638\n",
      "Train Loss 500 : 0.3416544019789277\n",
      "Train Loss 1000 : 0.3445436823126915\n",
      "Train Loss 1500 : 0.34512508954408566\n",
      "Train Loss 2000 : 0.3448518571080356\n",
      "Train Loss 2500 : 0.3465442514015002\n",
      "Val Loss : 0.4001968900283305, Val Accuracy : 0.821\n"
     ]
    }
   ],
   "source": [
    "classifier=SentenceClassifier(\n",
    "    n_vocab=n_vocab,hidden_dim=hidden_dim,embedding_dim=embedding_dim,\n",
    "    n_layers=n_layers,pretrained_embedding=init_embeddings\n",
    ").to(device)\n",
    "criterion=nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer=optim.RMSprop(classifier.parameters(),lr=0.001)\n",
    "\n",
    "epochs=5\n",
    "interval=500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(classifier,train_loader,criterion,optimizer,device,interval)\n",
    "    test(classifier,test_loader,criterion,device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TEXT_018_230_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
